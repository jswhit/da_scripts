#!/bin/sh
export OMP_NUM_THREADS=${OMP_NUM_THREADS:-1}
ulimit -s unlimited
[ -z "$nprocs" ] && echo "Need to set nprocs" && exit 1;
[ -z "$PGM" ] && echo "Need to set PGM" && exit 1;
# use srun
totcores=`expr $nprocs \* $OMP_NUM_THREADS`
totnodes=`python -c "from __future__ import print_function; import math; print(int(math.ceil(float(${totcores})/${corespernode})))"`
count=`python -c "from __future__ import print_function; import math; print(int(math.floor(float(${corespernode})/${mpitaskspernode})))"` 
mpitaskspernode=`python -c "from __future__ import print_function; import math; print(int(math.ceil(float(${nprocs})/${totnodes})))"`
# -c: cpus per mpi task (number of threads per mpi task)
# -n: total number of mpi tasks
# -N: number of nodes to run on
# --ntasks-per-node:  mpi tasks on each node
echo "running srun -N $totnodes -n $nprocs -c $count --ntasks-per-node=$mpitaskspernode  --exclusive --cpu-bind=cores --verbose --export=ALL $PGM"
eval srun -N $totnodes -n $nprocs -c $count --ntasks-per-node=$mpitaskspernode --exclusive --cpu-bind=cores --verbose --export=ALL $PGM
rc=$?
echo "exiting runmpi..."
exit $rc
